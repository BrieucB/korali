#include "engine.hpp"
#include "modules/experiment/experiment.hpp"
#include "modules/solver/learner/deepSupervisor/deepSupervisor.hpp"
#include "sample/sample.hpp"
#include <omp.h>

__startNamespace__;

void __className__::initialize()
{
  // Getting problem pointer
  _problem = dynamic_cast<problem::SupervisedLearning *>(_k->_problem);

  // Don't reinitialize if experiment was already initialized
  if (_k->_isInitialized == true) return;

  // Check whether the minibatch size (N) can be divided by the requested concurrency
  if (_problem->_trainingBatchSize % _trainingConcurrency > 0) KORALI_LOG_ERROR("The training concurrency requested (%lu) does not divide the training mini batch size (%lu) perfectly.", _trainingConcurrency, _problem->_trainingBatchSize);

  // Determining batch sizes
  std::vector<size_t> batchSizes = { _problem->_trainingBatchSize, _problem->_inferenceBatchSize };

  // If parallelizing training, we need to support the split batch size
  if (_trainingConcurrency > 1) batchSizes.push_back(_problem->_trainingBatchSize / _trainingConcurrency);

  /*****************************************************************
   * Setting up Neural Networks
   *****************************************************************/

  // Configuring neural network's inputs
  knlohmann::json neuralNetworkConfig;
  neuralNetworkConfig["Type"] = "Neural Network";
  neuralNetworkConfig["Engine"] = _neuralNetworkEngine;
  neuralNetworkConfig["Timestep Count"] = _problem->_maxTimesteps;

  // Iterator for the current layer id
  size_t curLayer = 0;

  // Setting the number of input layer nodes as number of input vector size
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Input";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_inputSize;
  curLayer++;

  // Adding user-defined hidden layers
  for (size_t i = 0; i < _neuralNetworkHiddenLayers.size(); i++)
  {
    neuralNetworkConfig["Layers"][curLayer]["Weight Scaling"] = _outputWeightsScaling;
    neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkHiddenLayers[i];
    curLayer++;
  }

  // Adding linear transformation layer to convert hidden state to match output channels
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Linear";
  neuralNetworkConfig["Layers"][curLayer]["Output Channels"] = _problem->_solutionSize;
  neuralNetworkConfig["Layers"][curLayer]["Weight Scaling"] = _outputWeightsScaling;
  curLayer++;

  // Applying a user-defined pre-activation function
  if (_neuralNetworkOutputActivation != "Identity")
  {
    neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Activation";
    neuralNetworkConfig["Layers"][curLayer]["Function"] = _neuralNetworkOutputActivation;
    curLayer++;
  }

  // Applying output layer configuration
  neuralNetworkConfig["Layers"][curLayer] = _neuralNetworkOutputLayer;
  neuralNetworkConfig["Layers"][curLayer]["Type"] = "Layer/Output";

  // Instancing training neural network
  auto trainingNeuralNetworkConfig = neuralNetworkConfig;
  trainingNeuralNetworkConfig["Batch Sizes"] = batchSizes;
  trainingNeuralNetworkConfig["Mode"] = "Training";
  _neuralNetwork = dynamic_cast<NeuralNetwork *>(getModule(trainingNeuralNetworkConfig, _k));
  _neuralNetwork->applyModuleDefaults(trainingNeuralNetworkConfig);
  _neuralNetwork->setConfiguration(trainingNeuralNetworkConfig);
  _neuralNetwork->initialize();

  /*****************************************************************
   * Initializing NN hyperparameters
   *****************************************************************/

  // If the hyperparameters have not been specified, produce new initial ones
  if (_hyperparameters.size() == 0) _hyperparameters = _neuralNetwork->generateInitialHyperparameters();

  /*****************************************************************
   * Setting up weight and bias optimization experiment
   *****************************************************************/

  if (_neuralNetworkOptimizer == "Adam") _optimizer = new korali::fAdam(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "AdaBelief") _optimizer = new korali::fAdaBelief(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "MADGRAD") _optimizer = new korali::fMadGrad(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "RMSProp") _optimizer = new korali::fRMSProp(_hyperparameters.size());
  if (_neuralNetworkOptimizer == "Adagrad") _optimizer = new korali::fAdagrad(_hyperparameters.size());

  // Setting hyperparameter structures in the neural network and optmizer
  setHyperparameters(_hyperparameters);

  // Resetting Optimizer
  _optimizer->reset();

  // Setting current loss
  _currentLoss = 0.0f;
}

void __className__::runGeneration()
{
  // Grabbing constants
  const size_t N = _problem->_trainingBatchSize;
  const size_t OC = _problem->_solutionSize;

  // Check whether training concurrency exceeds the number of workers
  if (_trainingConcurrency > _k->_engine->_conduit->getWorkerCount()) KORALI_LOG_ERROR("The training concurrency requested (%lu) exceeds the number of Korali workers defined in the conduit type/configuration (%lu).", _trainingConcurrency, _k->_engine->_conduit->getWorkerCount());

  // Updating solver's learning rate, if changed
  _optimizer->_eta = _learningRate;

  // Checking that incoming data has a correct format
  _problem->verifyData();

  // Hyperparameter gradient storage
  std::vector<float> nnHyperparameterGradients;

  // If we use an MSE loss function, we need to update the gradient vector with its difference with each of batch's last timestep of the NN output
  if (_lossFunction == "Mean Squared Error")
  {
    // Making a copy of the solution data for MSE calculation
    auto MSEVector = _problem->_solutionData;

    // Getting a reference to the neural network output
    const auto &results = getEvaluation(_problem->_inputData);

    // Calculating gradients via the loss function
#pragma omp parallel for simd
    for (size_t b = 0; b < N; b++)
      for (size_t i = 0; i < OC; i++)
       MSEVector[b][i] = MSEVector[b][i] - results[b][i];

    // Calculating loss across the batch size
    _currentLoss = 0.0;
    for (size_t b = 0; b < N; b++)
      for (size_t i = 0; i < OC; i++)
        _currentLoss += MSEVector[b][i] * MSEVector[b][i];
    _currentLoss = _currentLoss / ((float)N * 2.0f);

    // Running back propagation on the MSE vector
    nnHyperparameterGradients = backwardGradients(MSEVector);
  }

  // If the solution represents the gradients, just pass them on
  if (_lossFunction == "Direct Gradients")  nnHyperparameterGradients = backwardGradients(_problem->_solutionData);

  // Passing hyperparameter gradients through a gradient descent update
  _optimizer->processResult(0.0f, nnHyperparameterGradients);

  // Getting new set of hyperparameters from the gradient descent algorithm
  _neuralNetwork->setHyperparameters(_optimizer->_currentValue);
}

std::vector<float> __className__::getHyperparameters()
{
  return _neuralNetwork->getHyperparameters();
}

void __className__::setHyperparameters(const std::vector<float> &hyperparameters)
{
  // Update evaluation network
  _neuralNetwork->setHyperparameters(hyperparameters);

  // Updating optimizer's current value
  _optimizer->_currentValue = hyperparameters;
}

std::vector<std::vector<float>> &__className__::getEvaluation(const std::vector<std::vector<std::vector<float>>> &input)
{
  // Grabbing constants
  const size_t N = input.size();

  ///// We employ one of two ways to resolve the evaluation:

  // 1) If this is a training batch and concurrency was requested, we split the batch and send it to the Korali workers for processing
  if (N == _problem->_trainingBatchSize && _trainingConcurrency > 0)
  {
   // Clearing results vector
   _forwardEvaluation.clear();

   // Getting current NN hyperparameters
   const auto nnHyperparameters = _neuralNetwork->getHyperparameters();

   // Batch size per worker
   size_t batchSizePerWorker = _problem->_trainingBatchSize / _trainingConcurrency;

   // Sending input to workers for parallel processing
   std::vector<Sample> samples(_trainingConcurrency);
   for (size_t i = 0; i < _trainingConcurrency; i++)
   {
     // Carving part of the batch data that corresponds to this sample
     const auto beginWorkerData = _problem->_inputData.begin() + batchSizePerWorker * i;
     const auto endWorkerData = _problem->_inputData.begin() + batchSizePerWorker * (i+1);
     const auto workerInputData = std::vector<std::vector<std::vector<float>>>(beginWorkerData,endWorkerData);

     // Setting up sample
     samples[i]["Sample Id"] = i;
     samples[i]["Worker Affinity"] = i;
     samples[i]["Module"] = "Solver";
     samples[i]["Operation"] = "Run Evaluation On Worker";
     samples[i]["Input Data"] = workerInputData;
     samples[i]["Hyperparameters"] = nnHyperparameters;

     // Launching sample
     KORALI_START(samples[i]);
   }

   // Waiting for samples to finish
   KORALI_WAITALL(samples);

   for (size_t i = 0; i < _trainingConcurrency; i++)
   {
     const auto workerEvaluation = KORALI_GET(std::vector<std::vector<float>>, samples[i], "Evaluation");
     _forwardEvaluation.insert(_forwardEvaluation.end(), workerEvaluation.begin(), workerEvaluation.end());
   }

   return _forwardEvaluation;
  }

  // 2) If this is an inference batch or no concurrency was requested, we process it locally
  else
  {
   // Running the input values through the neural network
   _neuralNetwork->forward(input);

   // Returning the output values for the last given timestep
   return _neuralNetwork->getOutputValues(N);
  }
}

std::vector<float> &__className__::backwardGradients(const std::vector<std::vector<float>> &gradients)
{
  // Grabbing constants
  const size_t N = gradients.size();

  // 1) If this is a training batch and concurrency was requested, we split the batch and send it to the Korali workers for processing
  if (N == _problem->_trainingBatchSize && _trainingConcurrency > 1)
  {
   // Clearing results vector
   _hyperparameterGradients.clear();

   // Batch size per worker
   size_t batchSizePerWorker = _problem->_trainingBatchSize / _trainingConcurrency;

   // Sending input to workers for parallel processing
   std::vector<Sample> samples(_trainingConcurrency);

//   #pragma omp parallel for
   for (size_t i = 0; i < _trainingConcurrency; i++)
   {
     // Carving part of the batch data that corresponds to this sample
     const auto beginWorkerData = gradients.begin() + batchSizePerWorker * i;
     const auto endWorkerData = gradients.begin() + batchSizePerWorker * (i+1);
     const auto workerGradientData = std::vector<std::vector<float>>(beginWorkerData, endWorkerData);

     // Setting up sample
     samples[i]["Sample Id"] = i;
     samples[i]["Worker Affinity"] = i;
     samples[i]["Module"] = "Solver";
     samples[i]["Operation"] = "Run Backward Gradients On Worker";
     samples[i]["Gradient Data"] = workerGradientData;
   }

   // Launching Samples
   for (size_t i = 0; i < _trainingConcurrency; i++) KORALI_START(samples[i]);

   // Waiting for samples to finish
   KORALI_WAITALL(samples);

   _hyperparameterGradients = std::vector<float>(_neuralNetwork->_hyperparameterCount, 0.0f);
   for (size_t i = 0; i < _trainingConcurrency; i++)
   {
     const auto workerEvaluation = KORALI_GET(std::vector<float>, samples[i], "Hyperparameter Gradients");
     for (size_t i = 0; i < workerEvaluation.size(); i++) _hyperparameterGradients[i] += workerEvaluation[i];
   }
  }

  // 2) If this is an inference batch or no concurrency was requested, we process it locally
  else
  {
   // Running the input values through the neural network
   _neuralNetwork->backward(gradients);

   // Getting NN hyperparameter gradients
   _hyperparameterGradients = _neuralNetwork->getHyperparameterGradients(N);
  }

  // If required, apply L2 Normalization to the network's hyperparameters
  if (_l2RegularizationEnabled)
  {
    const auto nnHyperparameters = _neuralNetwork->getHyperparameters();
    #pragma omp parallel for simd
    for (size_t i = 0; i < _hyperparameterGradients.size(); i++)
     _hyperparameterGradients[i] -= _l2RegularizationImportance * nnHyperparameters[i];
  }

  // Returning the hyperparameter gradients
  return _hyperparameterGradients;
}

void __className__::runEvaluationOnWorker(korali::Sample &sample)
{
 // Updating hyperparameters in the worker's NN
 auto nnHyperparameters = KORALI_GET(std::vector<float>, sample, "Hyperparameters");
 _neuralNetwork->setHyperparameters(nnHyperparameters);
 sample._js.getJson().erase("Hyperparameters");

 // Getting input from sample
 auto input = KORALI_GET(std::vector<std::vector<std::vector<float>>>, sample, "Input Data");
 sample._js.getJson().erase("Input Data");

 // Grabbing batch size
 const size_t N = input.size();

 // Running the input values through the neural network
 _neuralNetwork->forward(input);

 // Storing the output values for the last given timestep
 sample["Evaluation"] = _neuralNetwork->getOutputValues(N);
}

void __className__::runBackwardGradientsOnWorker(korali::Sample &sample)
{
 // Getting input from sample
 auto gradients = KORALI_GET(std::vector<std::vector<float>>, sample, "Gradient Data");
 sample._js.getJson().erase("Gradient Data");

 // Grabbing batch size
 const size_t N = gradients.size();

 // Running the input values through the neural network
 _neuralNetwork->backward(gradients);

 // Storing the output values for the last given timestep
 sample["Hyperparameter Gradients"] = _neuralNetwork->getHyperparameterGradients(N);
}

void __className__::printGenerationAfter()
{
  // Printing results so far
  if (_lossFunction == "Mean Squared Error") _k->_logger->logInfo("Normal", " + Training Loss: %.15f\n", _currentLoss);
  if (_lossFunction == "Direct Gradient") _k->_logger->logInfo("Normal", " + Gradient L2-Norm: %.15f\n", std::sqrt(_currentLoss));
}

__moduleAutoCode__;

__endNamespace__;
